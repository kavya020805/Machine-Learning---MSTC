{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40c8a14"
      },
      "source": [
        "# Task (Feature Engineering Basics)\n",
        "Explore and apply various feature engineering techniques, including creating new numerical features and handling categorical variables through one-hot and label encoding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "038de48b"
      },
      "source": [
        "## Introduction to Feature Engineering\n",
        "\n",
        "### Subtask:\n",
        "Provide a brief introduction to what feature engineering is and its importance in machine learning workflows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a12813"
      },
      "source": [
        "### What is Feature Engineering?\n",
        "\n",
        "Feature engineering is the process of using domain knowledge to extract features (variables) from raw data. These features are then used to improve the performance of machine learning algorithms. It's about transforming raw data into a format that is more suitable for machine learning models to understand and learn from.\n",
        "\n",
        "### Why is Feature Engineering Important?\n",
        "\n",
        "1.  **Improved Model Performance**: Well-engineered features can significantly boost the accuracy and predictive power of machine learning models. Models often perform better when fed with relevant, well-structured features rather than raw, unprocessed data.\n",
        "2.  **Better Data Understanding**: The process of feature engineering often leads to a deeper understanding of the dataset and the problem at hand.\n",
        "3.  **Reduced Data Complexity**: It can help in reducing the dimensionality of the data by creating more informative features, thus simplifying the model and reducing overfitting.\n",
        "4.  **Handling Specific Data Types**: It allows for the conversion of various data types (e.g., categorical, date/time, text) into numerical formats that most machine learning algorithms can process.\n",
        "\n",
        "### How it Impacts Model Performance\n",
        "\n",
        "-   **Directly Influences Predictive Power**: A model's ability to learn patterns is highly dependent on the quality and relevance of its input features. Good features highlight the underlying structure of the data.\n",
        "-   **Reduces Overfitting/Underfitting**: Appropriate feature engineering can help create a balanced model. Too few or uninformative features can lead to underfitting, while too many redundant or noisy features can lead to overfitting.\n",
        "-   **Faster Training and Inference**: With a well-engineered, often more compact feature set, models can train faster and make predictions more efficiently.\n",
        "\n",
        "### Common Techniques and Examples\n",
        "\n",
        "-   **Handling Missing Values**: Imputing missing data using mean, median, mode, or more complex methods.\n",
        "-   **Encoding Categorical Variables**: Converting categories into numerical representations (e.g., One-Hot Encoding, Label Encoding).\n",
        "-   **Discretization/Binning**: Grouping continuous numerical data into bins (e.g., age ranges).\n",
        "-   **Scaling/Normalization**: Rescaling numerical features to a standard range (e.g., Min-Max Scaling, Standardization).\n",
        "-   **Creating Interaction Features**: Combining existing features to create new ones that capture relationships (e.g., `age * income`).\n",
        "-   **Extracting Time-Based Features**: Deriving features from datetime columns like day of the week, month, year, or time differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a9adefa"
      },
      "source": [
        "## Creating New Numerical Features\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate how to create new numerical features from existing ones using Python and pandas, including explanations for the code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69891ffc"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to convert the list of dictionaries into a pandas DataFrame, as requested. Then, create the 'price_per_room' feature by dividing 'price' by 'rooms', and display the head of the DataFrame to show the new feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "2c8db409",
        "outputId": "ffc74e65-c00f-407b-b0fc-528c6ebb3a98"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df['price_per_room'] = df['price'] / df['rooms']\n",
        "df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    price  rooms neighborhood  price_per_room\n",
              "0  850000      4   Queen Anne   212500.000000\n",
              "1  700000      3      Fremont   233333.333333\n",
              "2  650000      3  Wallingford   216666.666667\n",
              "3  600000      2      Fremont   300000.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4920d666-f26a-433e-b120-91eec1ceebc6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>rooms</th>\n",
              "      <th>neighborhood</th>\n",
              "      <th>price_per_room</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>850000</td>\n",
              "      <td>4</td>\n",
              "      <td>Queen Anne</td>\n",
              "      <td>212500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700000</td>\n",
              "      <td>3</td>\n",
              "      <td>Fremont</td>\n",
              "      <td>233333.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>650000</td>\n",
              "      <td>3</td>\n",
              "      <td>Wallingford</td>\n",
              "      <td>216666.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>600000</td>\n",
              "      <td>2</td>\n",
              "      <td>Fremont</td>\n",
              "      <td>300000.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4920d666-f26a-433e-b120-91eec1ceebc6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4920d666-f26a-433e-b120-91eec1ceebc6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4920d666-f26a-433e-b120-91eec1ceebc6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108012,\n        \"min\": 600000,\n        \"max\": 850000,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          700000,\n          600000,\n          850000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neighborhood\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Queen Anne\",\n          \"Fremont\",\n          \"Wallingford\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_per_room\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40593.82707132079,\n        \"min\": 212500.0,\n        \"max\": 300000.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          233333.33333333334,\n          300000.0,\n          212500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dad7a74"
      },
      "source": [
        "The code above demonstrates how to create a new numerical feature named `price_per_room`.\n",
        "\n",
        "1.  **`import pandas as pd`**: This line imports the pandas library, which is essential for working with DataFrames in Python.\n",
        "2.  **`df = pd.DataFrame(data)`**: This converts the provided `data` list of dictionaries into a pandas DataFrame and assigns it to the variable `df`.\n",
        "3.  **`df['price_per_room'] = df['price'] / df['rooms']`**: This is the core of the feature engineering step. It creates a new column in the DataFrame called `price_per_room`. The values in this new column are calculated by dividing the `price` of each property by its corresponding `rooms` count.\n",
        "4.  **`df.head()`**: This displays the first few rows of the DataFrame, allowing us to quickly verify that the new `price_per_room` column has been successfully added and calculated correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1075d73c"
      },
      "source": [
        "## Handling Categorical Variables: One-Hot Encoding\n",
        "\n",
        "### Subtask:\n",
        "Explain what categorical variables are and demonstrate how to perform one-hot encoding using Python and pandas, along with explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ad19e96"
      },
      "source": [
        "### What are Categorical Variables?\n",
        "\n",
        "Categorical variables are types of variables that divide the data into categories or groups. These categories can be either nominal (without a specific order) or ordinal (with a meaningful order). Unlike numerical variables, categorical variables represent qualities or characteristics that cannot be measured numerically.\n",
        "\n",
        "**Examples of Categorical Variables:**\n",
        "*   **Nominal:** Colors (Red, Blue, Green), Marital Status (Single, Married, Divorced), `neighborhood` (Queen Anne, Fremont, Wallingford).\n",
        "*   **Ordinal:** Education Level (High School, Bachelor's, Master's, PhD), Satisfaction Rating (Low, Medium, High).\n",
        "\n",
        "Machine learning algorithms often require numerical input, so categorical variables must be converted into a numerical format before they can be used in models. One-hot encoding is a common technique for this conversion, especially for nominal categorical variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74489e1"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that categorical variables are explained, the next step is to apply one-hot encoding using `pd.get_dummies()` to the 'neighborhood' column of the `df` DataFrame and display the result, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "34a3a8d8",
        "outputId": "9780ef02-ffff-429d-9826-d536f45ad8b9"
      },
      "source": [
        "df_encoded = pd.get_dummies(df, columns=['neighborhood'], prefix='neighborhood')\n",
        "df_encoded.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    price  rooms  price_per_room  neighborhood_Fremont  \\\n",
              "0  850000      4   212500.000000                 False   \n",
              "1  700000      3   233333.333333                  True   \n",
              "2  650000      3   216666.666667                 False   \n",
              "3  600000      2   300000.000000                  True   \n",
              "\n",
              "   neighborhood_Queen Anne  neighborhood_Wallingford  \n",
              "0                     True                     False  \n",
              "1                    False                     False  \n",
              "2                    False                      True  \n",
              "3                    False                     False  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8d92fc7-e5f3-440f-ae82-40a1b676d5fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>rooms</th>\n",
              "      <th>price_per_room</th>\n",
              "      <th>neighborhood_Fremont</th>\n",
              "      <th>neighborhood_Queen Anne</th>\n",
              "      <th>neighborhood_Wallingford</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>850000</td>\n",
              "      <td>4</td>\n",
              "      <td>212500.000000</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700000</td>\n",
              "      <td>3</td>\n",
              "      <td>233333.333333</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>650000</td>\n",
              "      <td>3</td>\n",
              "      <td>216666.666667</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>600000</td>\n",
              "      <td>2</td>\n",
              "      <td>300000.000000</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8d92fc7-e5f3-440f-ae82-40a1b676d5fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d8d92fc7-e5f3-440f-ae82-40a1b676d5fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d8d92fc7-e5f3-440f-ae82-40a1b676d5fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_encoded",
              "summary": "{\n  \"name\": \"df_encoded\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108012,\n        \"min\": 600000,\n        \"max\": 850000,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          700000,\n          600000,\n          850000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_per_room\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40593.82707132079,\n        \"min\": 212500.0,\n        \"max\": 300000.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          233333.33333333334,\n          300000.0,\n          212500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neighborhood_Fremont\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neighborhood_Queen Anne\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neighborhood_Wallingford\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b1ecdce"
      },
      "source": [
        "The code above demonstrates how to apply one-hot encoding to the `neighborhood` column using pandas' `get_dummies()` function.\n",
        "\n",
        "1.  **`df_encoded = pd.get_dummies(df, columns=['neighborhood'], prefix='neighborhood')`**:\n",
        "    *   `pd.get_dummies()` is a pandas function that converts categorical variable(s) into dummy/indicator variables (one-hot encoded).\n",
        "    *   `df`: This is the input DataFrame containing the data, including the `neighborhood` column.\n",
        "    *   `columns=['neighborhood']`: This argument specifies which column(s) from the DataFrame should be one-hot encoded. In this case, we are targeting the 'neighborhood' column.\n",
        "    *   `prefix='neighborhood'`: This argument adds a prefix to the new columns created from the one-hot encoding. For instance, 'Queen Anne' becomes 'neighborhood_Queen Anne', 'Fremont' becomes 'neighborhood_Fremont', and so on. This makes the new column names more descriptive and easier to understand.\n",
        "    *   The original `neighborhood` column is dropped by default once the new dummy variables are created.\n",
        "    *   The result, which is a new DataFrame with the one-hot encoded columns, is assigned to `df_encoded`.\n",
        "\n",
        "2.  **`df_encoded.head()`**: This displays the first few rows of the `df_encoded` DataFrame, allowing us to see the newly created one-hot encoded columns and verify their presence.\n",
        "\n",
        "### Why One-Hot Encoding is Beneficial:\n",
        "\n",
        "*   **Machine Learning Compatibility**: Most machine learning algorithms cannot directly work with categorical data. One-hot encoding converts these categories into a numerical format that algorithms can process.\n",
        "*   **Avoids Ordinality Assumption**: For nominal categorical variables (like `neighborhood` where there's no inherent order), simply assigning numerical labels (e.g., 0, 1, 2) can mislead the model into assuming an ordinal relationship (e.g., that 'Fremont' is 'greater' than 'Queen Anne'). One-hot encoding avoids this by treating each category as an independent binary feature.\n",
        "*   **Improved Model Performance**: By representing categorical features in a way that accurately reflects their nature, models can often learn more effectively and achieve better performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64019cc7"
      },
      "source": [
        "## Handling Categorical Variables: Label Encoding\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate how to perform label encoding using Python and scikit-learn, explaining its use cases and differences from one-hot encoding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31a98828"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to create a new ordinal categorical column in the existing DataFrame (`df`) as a prerequisite for demonstrating label encoding. This column will represent a hypothetical 'size_category' with ordinal values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "3fd5727f",
        "outputId": "20a77b1f-b240-4334-ff7f-9e7aee0e2847"
      },
      "source": [
        "df['size_category'] = ['Medium', 'Large', 'Small', 'Medium']\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    price  rooms neighborhood  price_per_room size_category\n",
              "0  850000      4   Queen Anne   212500.000000        Medium\n",
              "1  700000      3      Fremont   233333.333333         Large\n",
              "2  650000      3  Wallingford   216666.666667         Small\n",
              "3  600000      2      Fremont   300000.000000        Medium"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fe06f273-3911-4ca8-88d1-20a3156997b1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>rooms</th>\n",
              "      <th>neighborhood</th>\n",
              "      <th>price_per_room</th>\n",
              "      <th>size_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>850000</td>\n",
              "      <td>4</td>\n",
              "      <td>Queen Anne</td>\n",
              "      <td>212500.000000</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>700000</td>\n",
              "      <td>3</td>\n",
              "      <td>Fremont</td>\n",
              "      <td>233333.333333</td>\n",
              "      <td>Large</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>650000</td>\n",
              "      <td>3</td>\n",
              "      <td>Wallingford</td>\n",
              "      <td>216666.666667</td>\n",
              "      <td>Small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>600000</td>\n",
              "      <td>2</td>\n",
              "      <td>Fremont</td>\n",
              "      <td>300000.000000</td>\n",
              "      <td>Medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe06f273-3911-4ca8-88d1-20a3156997b1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fe06f273-3911-4ca8-88d1-20a3156997b1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fe06f273-3911-4ca8-88d1-20a3156997b1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 108012,\n        \"min\": 600000,\n        \"max\": 850000,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          700000,\n          600000,\n          850000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rooms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          4,\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"neighborhood\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Queen Anne\",\n          \"Fremont\",\n          \"Wallingford\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price_per_room\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40593.82707132079,\n        \"min\": 212500.0,\n        \"max\": 300000.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          233333.33333333334,\n          300000.0,\n          212500.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size_category\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Medium\",\n          \"Large\",\n          \"Small\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1883d19"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the 'size_category' column is available, the next step is to perform label encoding. This involves importing LabelEncoder, instantiating it, fitting it to the 'size_category' column, transforming it, and then displaying the DataFrame with the new encoded column.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f9dec44",
        "outputId": "c8d46c84-bb7c-4a45-d0fe-5bc2e54a69a7"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Instantiate LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the 'size_category' column\n",
        "df['size_encoded'] = label_encoder.fit_transform(df['size_category'])\n",
        "\n",
        "# Display the DataFrame with the new encoded column and the mapping\n",
        "print(\"DataFrame with Encoded Column:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nMapping of Categories to Encoded Values:\")\n",
        "for i, item in enumerate(label_encoder.classes_):\n",
        "    print(f\"{item} : {i}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Encoded Column:\n",
            "    price  rooms neighborhood  price_per_room size_category  size_encoded\n",
            "0  850000      4   Queen Anne   212500.000000        Medium             1\n",
            "1  700000      3      Fremont   233333.333333         Large             0\n",
            "2  650000      3  Wallingford   216666.666667         Small             2\n",
            "3  600000      2      Fremont   300000.000000        Medium             1\n",
            "\n",
            "Mapping of Categories to Encoded Values:\n",
            "Large : 0\n",
            "Medium : 1\n",
            "Small : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6bc6324"
      },
      "source": [
        "### What is Label Encoding?\n",
        "\n",
        "Label Encoding is a technique used to convert categorical data into numerical data. In this method, each unique category in a categorical column is assigned an integer value. For example, if a column contains 'Small', 'Medium', and 'Large', Label Encoding might assign 'Small' to 0, 'Medium' to 1, and 'Large' to 2.\n",
        "\n",
        "**Advantages of Label Encoding:**\n",
        "*   **Simplicity**: It's very straightforward to implement.\n",
        "*   **Memory Efficiency**: It uses less memory compared to one-hot encoding, especially for categorical columns with many unique values, as it only adds one numerical column.\n",
        "\n",
        "**Disadvantages of Label Encoding:**\n",
        "*   **Introduction of Ordinality (False Relationship)**: The main drawback is that it assigns an arbitrary numerical order to categories. If the categories do not have a natural ordinal relationship (e.g., 'red', 'green', 'blue'), machine learning algorithms might interpret these numerical labels as having an order or hierarchy. For example, if 'red' is 0, 'green' is 1, and 'blue' is 2, the model might incorrectly assume 'blue' is 'greater' or 'more important' than 'red'. This can negatively impact model performance, especially with algorithms that calculate distances between features (e.g., K-Nearest Neighbors, Support Vector Machines).\n",
        "\n",
        "**Use Cases for Label Encoding:**\n",
        "*   **Ordinal Categorical Variables**: It is best suited for categorical variables that inherently have an order or hierarchy. In our example, 'Small', 'Medium', 'Large' have a natural order, so encoding them as 0, 1, 2 makes sense.\n",
        "*   **Tree-based Algorithms**: Algorithms like Decision Trees, Random Forests, and Gradient Boosting Machines are less sensitive to the arbitrary ordering introduced by Label Encoding. They can effectively split data based on these integer labels without assuming an ordinal relationship.\n",
        "*   **High Cardinality Categorical Variables (with caution)**: For variables with a very large number of unique categories, one-hot encoding can lead to a very high-dimensional dataset (many new columns), which might be computationally expensive or lead to the curse of dimensionality. In such cases, if the ordinal assumption is acceptable or can be mitigated, label encoding might be considered.\n",
        "\n",
        "### Label Encoding vs. One-Hot Encoding\n",
        "\n",
        "The key difference lies in how they handle the representation of categorical variables:\n",
        "\n",
        "*   **One-Hot Encoding**: Creates new binary (0 or 1) columns for each category. Each row will have a '1' in the column corresponding to its category and '0' in all other new columns. This method explicitly avoids creating an artificial sense of order or magnitude among categories. It is ideal for **nominal categorical variables** where no inherent order exists.\n",
        "\n",
        "*   **Label Encoding**: Assigns a unique integer to each category. This can introduce an unintended ordinal relationship. It is primarily used for **ordinal categorical variables** where the order matters, or with specific machine learning models (like tree-based models) that are robust to this artificial ordering.\n",
        "\n",
        "**When to choose which:**\n",
        "*   Use **One-Hot Encoding** for nominal categorical variables (e.g., `neighborhood`, colors) to prevent the model from assuming an incorrect ordinal relationship.\n",
        "*   Use **Label Encoding** for ordinal categorical variables (e.g., `size_category`, education level) where the numerical order directly reflects the inherent order of the categories. For nominal variables, it should generally be avoided unless you are using algorithms (like tree-based models) that are not sensitive to ordinal relationships or if memory/dimensionality is a critical concern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "218fe861"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key concepts learned about feature engineering, new feature creation, and categorical variable handling, and provide suggestions for further exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fecfec24"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Feature Engineering Definition**: Feature engineering was defined as the process of extracting features from raw data using domain knowledge to improve machine learning model performance. Its importance was highlighted for boosting accuracy, enhancing data understanding, reducing complexity, and handling diverse data types.\n",
        "*   **Numerical Feature Creation**: A new numerical feature, `price_per_room`, was successfully created by dividing the `price` column by the `rooms` column. For example, a property priced at \\$850,000 with 4 rooms yielded a `price_per_room` of \\$212,500.\n",
        "*   **Categorical Variables Understanding**: Categorical variables were explained as data that divides observations into groups (nominal or ordinal), requiring conversion into numerical formats for machine learning models.\n",
        "*   **One-Hot Encoding Application**: One-hot encoding was applied to the 'neighborhood' column, converting it into multiple binary columns (e.g., `neighborhood_Fremont`, `neighborhood_Queen Anne`, `neighborhood_Wallingford`). This technique is crucial for nominal variables as it prevents the model from assuming an arbitrary ordinal relationship.\n",
        "*   **Label Encoding Application**: Label encoding was demonstrated on an ordinal `size_category` column (`Small`, `Medium`, `Large`), assigning unique integer values (e.g., 'Large' to 0, 'Medium' to 1, 'Small' to 2 based on alphabetical order). This method is memory-efficient but can introduce false ordinality if applied to nominal variables.\n",
        "*   **Encoding Method Selection**: The analysis emphasized that one-hot encoding is generally preferred for nominal categorical variables to avoid unintended ordinal relationships, while label encoding is suitable for truly ordinal variables or when using tree-based algorithms that are less sensitive to such an ordering.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The choice of feature engineering techniques, especially for categorical variables, is highly dependent on the data's nature (nominal vs. ordinal) and the machine learning model being used. Misapplication can lead to suboptimal model performance.\n",
        "*   Further exploration could involve applying more advanced feature engineering techniques such as polynomial features, interaction terms beyond simple ratios, or target encoding for high-cardinality categorical variables to potentially extract even more predictive power from the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52019920"
      },
      "source": [
        "# Task (Data Preprocessing)\n",
        "Continue exploring various feature engineering techniques, specifically focusing on handling binary categorical features (e.g., 'Yes'/'No') by converting them into numerical representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af967f79"
      },
      "source": [
        "## Introduction to Data Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Provide a brief introduction to data preprocessing and its importance in preparing data for machine learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8adff612"
      },
      "source": [
        "### What is Data Preprocessing?\n",
        "\n",
        "Data preprocessing is a crucial step in the data mining and machine learning pipeline that involves transforming raw data into an understandable and usable format. Raw data is often incomplete, inconsistent, and contains many errors or is in a format that cannot be directly fed into machine learning algorithms. Data preprocessing steps aim to clean, integrate, transform, reduce, and discretize data to make it suitable for analysis and model training.\n",
        "\n",
        "### Why is Data Preprocessing Important?\n",
        "\n",
        "1.  **Improved Model Performance**: Machine learning models perform best when fed with clean, consistent, and well-structured data. Preprocessing helps remove noise, handle missing values, and transform data into a suitable format, which directly leads to better model accuracy and generalization. Without it, models might learn incorrect patterns or fail to learn at all.\n",
        "2.  **Better Data Quality**: Raw data can suffer from various quality issues such as missing values, outliers, inconsistencies, and redundant features. Preprocessing techniques address these issues, resulting in higher quality data that is more reliable for analysis.\n",
        "3.  **Reduced Computational Cost**: By reducing data dimensionality (e.g., feature selection or extraction) and handling redundancies, preprocessing can significantly decrease the computational resources and time required for training models.\n",
        "4.  **Handling Diverse Data Types**: Machine learning algorithms typically require numerical input. Preprocessing converts various data types (e.g., categorical, text, date/time) into numerical representations that algorithms can understand.\n",
        "5.  **Preventing \"Garbage In, Garbage Out\"**: The principle \"Garbage In, Garbage Out\" (GIGO) applies strongly to machine learning. If the input data is of poor quality, the output from the model will also be of poor quality, regardless of how sophisticated the algorithm is. Preprocessing ensures that the input is as clean and relevant as possible.\n",
        "\n",
        "In essence, data preprocessing is about preparing data to maximize the chances of a machine learning model performing optimally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bd24e90"
      },
      "source": [
        "## Handling Binary Categorical Features (Yes/No)\n",
        "\n",
        "### Subtask:\n",
        "Demonstrate how to convert binary categorical features (like 'Yes'/'No') into numerical representations using Python and pandas, with code and explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b39fe42"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I will create the `df_binary` DataFrame with the `has_garage` column and then convert the 'Yes'/'No' values to numerical representation (1/0) in a new column `has_garage_encoded`. Finally, I will display the head of the DataFrame to show the result.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e3c1c73",
        "outputId": "92ac84d7-ee13-41df-d1be-d39a9c529f26"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Create a new pandas DataFrame with a binary categorical column\n",
        "df_binary = pd.DataFrame({'has_garage': ['Yes', 'No', 'Yes', 'No']})\n",
        "\n",
        "# 2. Convert 'Yes'/'No' to 1/0 using map\n",
        "df_binary['has_garage_encoded'] = df_binary['has_garage'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# 3. Display the first few rows of the df_binary DataFrame\n",
        "print(df_binary.head())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  has_garage  has_garage_encoded\n",
            "0        Yes                   1\n",
            "1         No                   0\n",
            "2        Yes                   1\n",
            "3         No                   0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75512e4f"
      },
      "source": [
        "The code above demonstrates how to convert a binary categorical feature into a numerical representation.\n",
        "\n",
        "1.  **`import pandas as pd`**: This line imports the pandas library, which is essential for creating and manipulating DataFrames.\n",
        "2.  **`df_binary = pd.DataFrame({'has_garage': ['Yes', 'No', 'Yes', 'No']})`**: This creates a new pandas DataFrame called `df_binary`. It has one column, `has_garage`, which contains binary categorical values ('Yes' and 'No').\n",
        "3.  **`df_binary['has_garage_encoded'] = df_binary['has_garage'].map({'Yes': 1, 'No': 0})`**: This is the core of the encoding process. The `.map()` method is used to apply a dictionary mapping to the `has_garage` column. For each value in `has_garage`:\n",
        "    *   'Yes' is replaced with `1`.\n",
        "    *   'No' is replaced with `0`.\n",
        "    The result is stored in a new column called `has_garage_encoded`.\n",
        "4.  **`print(df_binary.head())`**: This displays the first few rows of the modified `df_binary` DataFrame, showing both the original 'has_garage' column and the newly created numerical 'has_garage_encoded' column.\n",
        "\n",
        "### Benefits of Encoding Binary Categorical Features (Yes/No) to 1/0:\n",
        "\n",
        "*   **Machine Learning Compatibility**: Most machine learning algorithms require numerical input. Converting 'Yes'/'No' to `1`/`0` allows these algorithms to process the feature.\n",
        "*   **Simplicity and Interpretability**: This is a direct and intuitive way to represent binary states. `1` often signifies the presence of a characteristic (e.g., 'Yes', 'True'), and `0` signifies its absence (e.g., 'No', 'False'). This makes the model's interpretation of the feature straightforward.\n",
        "*   **No False Ordinality**: Unlike label encoding for multi-class nominal features, encoding binary features as `0` and `1` does not introduce an artificial ordinal relationship that could mislead a model. While `1` is numerically greater than `0`, the binary nature makes this a natural representation of presence/absence or one state versus another without implying a complex order.\n",
        "*   **Memory Efficiency**: It uses minimal memory, as only one numerical column is added to represent the binary information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b378ea8"
      },
      "source": [
        "## Applying Label Encoding (revisited)\n",
        "\n",
        "### Subtask:\n",
        "Revisit Label Encoding with a new example or context, demonstrating its application to an ordinal categorical feature, including code and explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f4649a"
      },
      "source": [
        "**Reasoning**:\n",
        "To demonstrate label encoding on a new ordinal categorical feature, I will create a new DataFrame with a 'quality_rating' column, apply LabelEncoder to it, and then display the results and the mapping of categories to their encoded values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6e07aeb",
        "outputId": "cbbad03e-a125-414f-be6c-76c15e2655f0"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Create a new pandas DataFrame with an ordinal categorical column\n",
        "df_quality = pd.DataFrame({'quality_rating': ['Good', 'Very Good', 'Poor', 'Good', 'Excellent', 'Poor']})\n",
        "\n",
        "# 2. Instantiate LabelEncoder\n",
        "label_encoder_quality = LabelEncoder()\n",
        "\n",
        "# 3. Fit and transform the 'quality_rating' column\n",
        "df_quality['quality_encoded'] = label_encoder_quality.fit_transform(df_quality['quality_rating'])\n",
        "\n",
        "# 4. Print the df_quality DataFrame's head\n",
        "print(\"DataFrame with Encoded Column:\")\n",
        "print(df_quality.head())\n",
        "\n",
        "# 5. Print the mapping of categories to encoded values\n",
        "print(\"\\nMapping of Categories to Encoded Values:\")\n",
        "for i, item in enumerate(label_encoder_quality.classes_):\n",
        "    print(f\"{item} : {i}\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Encoded Column:\n",
            "  quality_rating  quality_encoded\n",
            "0           Good                1\n",
            "1      Very Good                3\n",
            "2           Poor                2\n",
            "3           Good                1\n",
            "4      Excellent                0\n",
            "\n",
            "Mapping of Categories to Encoded Values:\n",
            "Excellent : 0\n",
            "Good : 1\n",
            "Poor : 2\n",
            "Very Good : 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb6668a9"
      },
      "source": [
        "The code above demonstrates how to apply Label Encoding to an ordinal categorical feature, `quality_rating`.\n",
        "\n",
        "1.  **`import pandas as pd`**: Imports the pandas library for DataFrame manipulation.\n",
        "2.  **`from sklearn.preprocessing import LabelEncoder`**: Imports the `LabelEncoder` class from scikit-learn, which is used for label encoding.\n",
        "3.  **`df_quality = pd.DataFrame({'quality_rating': ['Good', 'Very Good', 'Poor', 'Good', 'Excellent', 'Poor']})`**: A new DataFrame `df_quality` is created. It contains an ordinal categorical column `quality_rating` with values representing different quality levels.\n",
        "4.  **`label_encoder_quality = LabelEncoder()`**: An instance of `LabelEncoder` is created. This object will learn the unique categories and assign numerical labels to them.\n",
        "5.  **`df_quality['quality_encoded'] = label_encoder_quality.fit_transform(df_quality['quality_rating'])`**: This is the core step for encoding:\n",
        "    *   `.fit_transform()` is called on the `label_encoder_quality` object, passing the `quality_rating` column.\n",
        "    *   `fit()`: The encoder learns all unique categories in the `quality_rating` column and determines an integer mapping for each. By default, it assigns integers alphabetically.\n",
        "    *   `transform()`: The learned mapping is then applied to the `quality_rating` column, converting each categorical value into its corresponding integer. For example, 'Excellent' becomes 0, 'Good' becomes 1, 'Poor' becomes 2, and 'Very Good' becomes 3 (based on alphabetical order).\n",
        "    *   The resulting numerical array is assigned to a new column `quality_encoded` in the `df_quality` DataFrame.\n",
        "6.  **`print(\"DataFrame with Encoded Column:\")`** and **`print(df_quality.head())`**: These lines display the first few rows of the DataFrame, allowing a visual check of the new `quality_encoded` column alongside the original `quality_rating`.\n",
        "7.  **`print(\"\\nMapping of Categories to Encoded Values:\")`** and the subsequent loop:**\n",
        "    *   `label_encoder_quality.classes_` attribute holds the array of unique categories that the encoder learned, sorted alphabetically.\n",
        "    *   The loop iterates through these classes and their corresponding index (which is the assigned encoded value) to explicitly show the mapping (e.g., 'Excellent' : 0, 'Good' : 1, etc.).\n",
        "\n",
        "### Why use Label Encoding for Ordinal Features?\n",
        "\n",
        "As discussed previously, Label Encoding is particularly suitable for **ordinal categorical variables** because the inherent order among categories can sometimes be preserved by the numerical assignments. In this example, while `LabelEncoder` assigns based on alphabetical order ('Excellent' (0), 'Good' (1), 'Poor' (2), 'Very Good' (3)), it's important to remember that for true ordinality, the mapping might need to be explicitly defined if alphabetical order doesn't align with the desired rank (e.g., if 'Poor' should be 0, 'Good' 1, 'Very Good' 2, 'Excellent' 3). However, for many tree-based models, the exact numerical difference doesn't matter as much as the ability to split on different values, making Label Encoding a viable option for ordinal features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b83ed5c8"
      },
      "source": [
        "## Performing Train-Test Split\n",
        "\n",
        "### Subtask:\n",
        "Explain the concept of train-test split and demonstrate how to split a dataset into training and testing sets using scikit-learn, with code and explanations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "480a7780"
      },
      "source": [
        "### What is Train-Test Split?\n",
        "\n",
        "Train-test split is a fundamental data preprocessing and evaluation technique in machine learning. It involves dividing a dataset into two subsets: a training set and a testing (or validation) set.\n",
        "\n",
        "*   **Training Set**: This subset of the data is used to train the machine learning model. The model learns patterns, relationships, and parameters from this data.\n",
        "*   **Testing Set**: This subset is used to evaluate the performance of the trained model. It consists of data that the model has not seen during training, providing an unbiased assessment of how well the model generalizes to new, unseen data.\n",
        "\n",
        "### Why is Train-Test Split Important?\n",
        "\n",
        "1.  **Assessing Generalization**: The primary goal of a machine learning model is not just to perform well on the data it has seen (training data) but to generalize effectively to new, unseen data. Splitting the data into training and testing sets allows us to simulate this real-world scenario.\n",
        "2.  **Detecting Overfitting**: If a model performs exceptionally well on the training data but poorly on the testing data, it indicates overfitting. Overfitting occurs when a model learns the training data too well, including its noise and specific patterns, making it unable to generalize. The test set serves as a crucial check against this.\n",
        "3.  **Unbiased Evaluation**: By evaluating the model on a separate, unseen test set, we get a more reliable and unbiased estimate of its true performance. If we were to evaluate the model on the same data it was trained on, the performance metrics would likely be overly optimistic.\n",
        "4.  **Hyperparameter Tuning and Model Selection**: The test set helps in comparing different models or different configurations (hyperparameters) of the same model. We can train several models on the training data and then select the one that performs best on the test data.\n",
        "\n",
        "### How it's Used to Evaluate Model Performance\n",
        "\n",
        "After training a model on the training set, various metrics (e.g., accuracy, precision, recall, F1-score for classification; Mean Squared Error, R-squared for regression) are calculated on the test set. These metrics quantify how well the model makes predictions on data it hasn't encountered before. A robust model will show consistent performance across both training and testing sets, indicating good generalization capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a671a114"
      },
      "source": [
        "**Reasoning**:\n",
        "The next step is to generate a sample dataset, import `train_test_split`, split the dataset into training and testing sets, and then print the shapes of these sets, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b6fce46",
        "outputId": "db8b141e-b212-46ab-cd9a-6d670f29bfbf"
      },
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Generate a sample dataset for a classification task\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, # 1000 samples\n",
        "    n_features=20,  # 20 features\n",
        "    n_informative=10, # 10 informative features\n",
        "    n_redundant=5,  # 5 redundant features\n",
        "    n_classes=2,    # Binary classification\n",
        "    random_state=42 # for reproducibility\n",
        ")\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Print the shapes of the resulting training and testing sets\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (800, 20)\n",
            "Shape of X_test: (200, 20)\n",
            "Shape of y_train: (800,)\n",
            "Shape of y_test: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc1cbc9"
      },
      "source": [
        "The code above demonstrates how to perform a train-test split on a generated dataset.\n",
        "\n",
        "1.  **`from sklearn.datasets import make_classification`**:\n",
        "    *   Imports the `make_classification` function from scikit-learn. This utility is used to generate a synthetic dataset for classification tasks, complete with features (X) and a target variable (y).\n",
        "\n",
        "2.  **`from sklearn.model_selection import train_test_split`**:\n",
        "    *   Imports the `train_test_split` function, a crucial tool from scikit-learn for splitting datasets.\n",
        "\n",
        "3.  **`X, y = make_classification(...)`**:\n",
        "    *   This line generates our sample dataset.\n",
        "    *   `n_samples=1000`: Creates a dataset with 1000 observations (rows).\n",
        "    *   `n_features=20`: Each observation will have 20 features (columns in X).\n",
        "    *   `n_informative=10`: 10 of these features are actually useful for predicting the target variable.\n",
        "    *   `n_redundant=5`: 5 features are linear combinations of the informative features.\n",
        "    *   `n_classes=2`: The target variable `y` will have two classes (binary classification).\n",
        "    *   `random_state=42`: Ensures that the dataset generation is reproducible. Running this code again with the same `random_state` will produce the exact same dataset.\n",
        "    *   `X` stores the features, and `y` stores the target variable.\n",
        "\n",
        "4.  **`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)`**:\n",
        "    *   This is the core of the train-test split operation.\n",
        "    *   `X, y`: These are the features and target variable from our generated dataset that we want to split.\n",
        "    *   `test_size=0.2`: Specifies that 20% of the data should be used for the test set, and the remaining 80% for the training set. You can also specify `train_size`.\n",
        "    *   `random_state=42`: Similar to `make_classification`, this parameter ensures reproducibility of the split. If you run the code again, the same data points will end up in the training and testing sets.\n",
        "    *   `stratify=y`: This is particularly important for classification tasks. It ensures that the proportion of samples for each class is roughly the same in both the training and testing sets as it is in the complete dataset. For example, if 70% of the original `y` are class 0 and 30% are class 1, then `y_train` and `y_test` will also have approximately these proportions. This helps prevent skewed class distributions in small datasets.\n",
        "    *   The function returns four outputs: `X_train` (training features), `X_test` (testing features), `y_train` (training target), and `y_test` (testing target).\n",
        "\n",
        "5.  **`print(f\"Shape of X_train: {X_train.shape}\")` (and similar lines)**:\n",
        "    *   These lines print the dimensions (number of rows, number of columns) of each resulting set.\n",
        "    *   From the output:\n",
        "        *   `X_train: (800, 20)`: 800 samples for training, each with 20 features.\n",
        "        *   `X_test: (200, 20)`: 200 samples for testing, each with 20 features (which is 20% of 1000 samples).\n",
        "        *   `y_train: (800,)`: 800 corresponding target values for the training set.\n",
        "        *   `y_test: (200,)`: 200 corresponding target values for the testing set.\n",
        "\n",
        "This confirms that the dataset has been successfully split into appropriate training and testing subsets, ready for model training and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5b3f4a9"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the key concepts learned about data preprocessing, handling binary and ordinal categorical features, and performing a train-test split, providing suggestions for further exploration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ae6cfba"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Importance of Data Preprocessing**: Data preprocessing is a vital step in machine learning workflows, transforming raw data into an understandable format. It is crucial for improving model performance, ensuring high data quality, reducing computational costs, handling diverse data types, and preventing \"Garbage In, Garbage Out\" scenarios.\n",
        "*   **Binary Categorical Feature Encoding**: Binary categorical features (e.g., 'Yes'/'No') were effectively converted into a numerical 1/0 representation using the pandas `.map()` method. This approach ensures compatibility with machine learning models, maintains simplicity, avoids introducing false ordinality, and is memory-efficient. For example, 'Yes' was mapped to 1, and 'No' to 0.\n",
        "*   **Ordinal Categorical Feature Encoding**: Label Encoding was applied to an ordinal categorical feature, `quality_rating` (e.g., 'Good', 'Very Good', 'Poor', 'Excellent'), using `sklearn.preprocessing.LabelEncoder`. The encoder assigned numerical values based on alphabetical order (e.g., 'Excellent' to 0, 'Good' to 1, 'Poor' to 2, 'Very Good' to 3). While suitable for ordinal data, it was noted that explicit mapping might be necessary if the alphabetical order does not align with the desired inherent rank.\n",
        "*   **Train-Test Split Implementation**: The `train_test_split` function from `sklearn.model_selection` was demonstrated to divide a dataset into training and testing sets. For a synthetic dataset of 1000 samples, 80% (800 samples) were allocated to the training set and 20% (200 samples) to the testing set. The `stratify=y` parameter was used to ensure that the class distribution of the target variable was preserved in both the training and testing sets, which is critical for classification tasks.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The selection of an appropriate encoding strategy is crucial and depends on the nature of the categorical variable (binary, ordinal, or nominal) to prevent introducing misleading information to the model.\n",
        "*   Further exploration should include demonstrating `OneHotEncoder` for nominal categorical features with more than two unique values, to avoid implicit ordinal relationships that `LabelEncoder` can create.\n"
      ]
    }
  ]
}